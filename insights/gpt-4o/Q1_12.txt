The paper suggests analyzing sentiment in software engineering texts by leveraging large language models (LLMs), specifically focusing on bigger LLMs (bLLMs) and smaller LLMs (sLLMs). The study highlights the effectiveness of bLLMs in scenarios with limited labeled data and imbalanced datasets, where they can achieve state-of-the-art performance even in zero-shot settings. For datasets with ample and balanced training data, fine-tuning sLLMs is recommended. The paper emphasizes the importance of prompt engineering for bLLMs, as different prompts can significantly impact performance. It also suggests that few-shot learning can enhance bLLM performance, although the improvement may not always be substantial. The study underscores the need for consistent labeling rules and guidelines to improve sentiment analysis accuracy. Overall, the approach involves selecting the appropriate model and strategy based on the dataset characteristics and employing effective prompt engineering to optimize performance.